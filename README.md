# ML-practice
Self practice based on <a href="https://www.books.com.tw/products/0010859473">this book</a>

<div><p>1.0 Supervised Learning<p/>
  <p>1.1 Multiple Linear Regression (package: LinearRegression,Ridge,KNeighborsRegressor) <br/>
     - with regularization: Lasso, Ridge (.isin([?], .assign(price = pd.to_numeric(auto.price))<p/>
  <p>1.2 Logistic Regression (package: LogisticRegression)<br/>
     - with normalization, standardization (.map(lambda x:1 if x == '>50K' else 0)<p/>
  <p>1.3 k-Nearest Neighbors (kNN) (package: KNeighborsClassifier)<p/>
      1.3.1 Recommendation System (kNN application) <br/>
      1.3.1.1 Content Based Filtering<br/>
      1.3.1.2 Collaborative Filtering <br/>
      <blockquote>a. Memory-Based: User-User Similarity, Item-Item Similarity<br/>  
        b. Model-Based:k-Nearest Neighbors(KNN), Matrix Facorization: Singular Value Decomposition (SVD)</blockquote><br/>
 <p> 1.4. Decision Tree<br/><p/>
   <p>1.5. Support Vector Machine (SVM)<br/><p/>
<p>2.0 Unsupervised Learning<br/><p/>
  <p>2.1 Cluster Analysis (k-means)<br/><p/>
  <p>2.2 Principal Component Analysis<br/><p/>
  <p>2.3 Basket Analysis<br/><p/>
<p>3.0 <a href="https://xijunlee.github.io/2017/06/03/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">Ensemble: Bagging, Boosting, Stacking</a><br/><p/>
  <p>3.1 Bagging: Random Forest<br/><p/>
  <p>3.2 Booosting: AdaBoost (adaptive Boosting), GBDT (Gradient Boosting Decision Tree), XGBoost<p/><div/>
  
